{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eba1073a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, concatenate\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "818751a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load Spacy model\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e24388b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\getty\\Desktop\\my data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a48e8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\getty\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Data preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4e79cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"label\"] == \"Spam\", \"label\"] = 1.0\n",
    "data.loc[data[\"label\"] == \"Non-Spam\", \"label\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "288b1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ef0fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39masarray(y_train)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y_test)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(y_train).astype(\"float64\")\n",
    "y_test = np.asarray(y_test).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7575248",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "num_filters = 128\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f6e37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_words = 10000\n",
    "max_len = 200\n",
    "max_chars = 200\n",
    "embedding_dim = 128  # example value\n",
    "num_filters = 64    # example value\n",
    "dropout_rate = 0.5  # example value\n",
    "\n",
    "# Assume x_train and x_test are already defined and preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0296538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_features(text):\n",
    "    doc = nlp(text)\n",
    "    syntax = len([token.dep_ for token in doc])\n",
    "    semantic = len([token.ent_type_ for token in doc])\n",
    "    length_of_text = len(text)\n",
    "    presence_of_hyperlinks = int('http' in text or 'www' in text)\n",
    "    return [syntax, semantic, length_of_text, presence_of_hyperlinks]\n",
    "\n",
    "# Extract content-based features for training and testing sets\n",
    "x_train_content_features = np.array([extract_content_features(text) for text in x_train])\n",
    "x_test_content_features = np.array([extract_content_features(text) for text in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa56d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract character-based features\n",
    "def extract_character_features(text):\n",
    "    num_chars = len(text)\n",
    "    repeated_chars = len([char for char in text if text.count(char) > 1])\n",
    "    capitalized_words = len([word for word in text.split() if word.isupper()])\n",
    "    freq_specific_chars = len([char for char in text if char in '@#$%^&*'])\n",
    "    return [num_chars, repeated_chars, capitalized_words, freq_specific_chars]\n",
    "\n",
    "# Extract character-based features for training and testing sets\n",
    "x_train_char_features = np.array([extract_character_features(text) for text in x_train])\n",
    "x_test_char_features = np.array([extract_character_features(text) for text in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f22cc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and padding for text sequences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_train_padded = pad_sequences(x_train_seq, maxlen=max_len)\n",
    "\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_padded = pad_sequences(x_test_seq, maxlen=max_len)\n",
    "\n",
    "# Tokenization and padding for character sequences\n",
    "def char_tokenizer(text):\n",
    "    return [ord(char) for char in text]\n",
    "\n",
    "x_train_char_seq = x_train.apply(char_tokenizer)\n",
    "x_test_char_seq = x_test.apply(char_tokenizer)\n",
    "\n",
    "x_train_char_padded = pad_sequences(x_train_char_seq, maxlen=max_chars)\n",
    "x_test_char_padded = pad_sequences(x_test_char_seq, maxlen=max_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bb90697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the data types are consistent\n",
    "x_train_padded = np.asarray(x_train_padded.astype(\"float64\"))\n",
    "x_test_padded = np.asarray(x_test_padded.astype(\"float64\"))\n",
    "x_train_char_padded = np.asarray(x_train_char_padded.astype(\"float64\"))\n",
    "x_test_char_padded = np.asarray(x_test_char_padded.astype(\"float64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "852aab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the 7 samples in x_train_char_padded to match the 11156 samples in x_train_padded\n",
    "num_repeats = len(x_train_padded) // len(x_train_char_padded)\n",
    "x_train_char_padded_repeated = np.repeat(x_train_char_padded, num_repeats, axis=0)\n",
    "\n",
    "# Repeat the samples in x_train_char_padded to match 11151 samples in x_train_padded\n",
    "num_additional_samples = len(x_train_padded) - len(x_train_char_padded)\n",
    "additional_samples_indices = np.random.choice(len(x_train_char_padded), num_additional_samples)\n",
    "x_train_char_padded_adj = np.concatenate((x_train_char_padded, x_train_char_padded[additional_samples_indices]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6419443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the 7 samples in x_train_char_padded to match the 11156 samples in x_train_padded\n",
    "num_repeats = len(x_test_padded) // len(x_test_char_padded)\n",
    "x_test_char_padded_repeated = np.repeat(x_test_char_padded, num_repeats, axis=0)\n",
    "\n",
    "# Repeat the samples in x_train_char_padded to match 11151 samples in x_train_padded\n",
    "num_additional_samples = len(x_test_padded) - len(x_test_char_padded)\n",
    "additional_samples_indices = np.random.choice(len(x_test_char_padded), num_additional_samples)\n",
    "x_test_char_padded_adj = np.concatenate((x_test_char_padded, x_test_char_padded[additional_samples_indices]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f08e32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging content and character-based features with the padded sequences\n",
    "x_train_combined = np.hstack([x_train_padded, x_train_content_features])\n",
    "x_test_combined = np.hstack([x_test_padded, x_test_content_features])\n",
    "\n",
    "# Model definitions\n",
    "content_model = Sequential()\n",
    "content_model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "content_model.add(Conv1D(num_filters, 5, activation='relu'))\n",
    "content_model.add(GlobalMaxPooling1D())\n",
    "content_model.add(Dropout(dropout_rate))\n",
    "\n",
    "char_model = Sequential()\n",
    "char_model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_chars))\n",
    "char_model.add(Conv1D(num_filters, 5, activation='relu'))\n",
    "char_model.add(GlobalMaxPooling1D())\n",
    "char_model.add(Dropout(dropout_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4baf1f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merging models\n",
    "merged = concatenate([content_model.output, char_model.output])\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(dropout_rate)(merged)\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# Final model\n",
    "model = tf.keras.Model(inputs=[content_model.input, char_model.input], outputs=output)\n",
    "\n",
    "# Compilation\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Output model summary\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b698daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)\n",
    "\n",
    "# Computing class weights manually\n",
    "class_weights = {cls: len(y_train) / (len(classes) * (y_train == cls).sum()) for cls in classes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "291b6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss (or choose 'val_accuracy' if you prefer)\n",
    "    min_delta=0.001,  # Minimum change to qualify as an improvement\n",
    "    patience=10,  # How many epochs to wait after last time validation loss improved\n",
    "    verbose=1,\n",
    "    mode='min',  # 'min' mode means training will stop when the quantity monitored has stopped decreasing\n",
    "    restore_best_weights=False  # Whether to restore model weights from the epoch with the best value of the monitored quantity.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "959f0313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 9.8483e-05 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 0.9989\n",
      "Epoch 2/100\n",
      "175/175 [==============================] - 16s 89ms/step - loss: 1.7273e-04 - accuracy: 0.9999 - val_loss: 0.0202 - val_accuracy: 0.9978\n",
      "Epoch 3/100\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 1.5272e-04 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 0.9975\n",
      "Epoch 4/100\n",
      "175/175 [==============================] - 16s 91ms/step - loss: 2.2839e-05 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 0.9975\n",
      "Epoch 5/100\n",
      "175/175 [==============================] - 15s 87ms/step - loss: 1.4956e-05 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 0.9975\n",
      "Epoch 6/100\n",
      "175/175 [==============================] - 15s 86ms/step - loss: 5.8117e-06 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 0.9975\n",
      "Epoch 7/100\n",
      "175/175 [==============================] - 15s 88ms/step - loss: 7.7470e-04 - accuracy: 0.9996 - val_loss: 0.0187 - val_accuracy: 0.9975\n",
      "Epoch 8/100\n",
      "175/175 [==============================] - 15s 86ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0107 - val_accuracy: 0.9982\n",
      "Epoch 9/100\n",
      "175/175 [==============================] - 16s 91ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0237 - val_accuracy: 0.9964\n",
      "Epoch 10/100\n",
      "175/175 [==============================] - 15s 87ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.0178 - val_accuracy: 0.9971\n",
      "Epoch 11/100\n",
      "175/175 [==============================] - 15s 86ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0197 - val_accuracy: 0.9971\n",
      "Epoch 11: early stopping\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.0197 - accuracy: 0.9971\n",
      "Test Loss: 0.01968015357851982\n",
      "Test Accuracy: 0.9971325993537903\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit(\n",
    "    [x_train_padded, x_train_char_padded_adj],  # Training data\n",
    "    y_train,  # Training labels\n",
    "    batch_size=batch_size,\n",
    "    epochs=100,  # Set to a high number since training will stop automatically\n",
    "    validation_data=([x_test_padded, x_test_char_padded_adj], y_test),  # or use validation_data=(x_val, y_val)\n",
    "    callbacks=[early_stopping],class_weight = class_weights  # Include the EarlyStopping callback\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate([x_test_padded, x_test_char_padded_adj], y_test, batch_size=batch_size)\n",
    "\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5cea20e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_probs = model.predict([x_test_padded, x_test_char_padded_adj], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6059f4bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 1s 12ms/step\n",
      "Precision: 0.9985315712187959\n",
      "Recall: 0.9956076134699854\n",
      "F1 Score: 0.9970674486803519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming x_test_padded and x_test_char_padded_adj are your test inputs\n",
    "# Get model's probability predictions for the positive class\n",
    "pred_probs = model.predict([x_test_padded, x_test_char_padded_adj])\n",
    "# Convert probabilities to binary predictions using 0.5 as the threshold\n",
    "pred_labels = np.where(pred_probs > 0.5, 1, 0)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, pred_labels)\n",
    "recall = recall_score(y_test, pred_labels)\n",
    "f1 = f1_score(y_test, pred_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daa9e69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 21ms/step\n",
      "F1 Score: 0.6498674120575614\n",
      "Precision: 0.5703227091121645\n",
      "Recall: 0.7551971326164875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\getty\\anaconda3\\envs\\gerty\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate predictions for the test data\n",
    "pred_probs = model.predict([x_test_padded, x_test_char_padded_adj], batch_size=batch_size)\n",
    "# Convert probabilities to class predictions (for multiclass, assuming one-hot encoding)\n",
    "pred = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# If your y_test is one-hot encoded, convert it back to class indices for comparison\n",
    "if y_test.ndim > 1:\n",
    "    y_test_indices = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_test_indices = y_test\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "f1 = f1_score(y_test_indices, pred, average='weighted')  # Use 'binary' for binary classification, 'macro' for multiclass, average='macro'\n",
    "precision = precision_score(y_test_indices, pred, average='weighted')\n",
    "recall = recall_score(y_test_indices, pred,average='weighted')\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(\"F1 Score:\",f1)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87b218e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 232ms/step\n",
      "Text: Congratulations! You've won a free cruise. Click the link to claim your prize. - Spam Probability: [0.02585566]\n",
      "Non-Spam\n",
      "Text: Hi there, just checking in. How are you doing? - Spam Probability: [0.00173024]\n",
      "Non-Spam\n",
      "Text: You are a great role model - Spam Probability: [0.00946098]\n",
      "Non-Spam\n",
      "Text: 100 dating service cal;l 09064012103 box334sk38ch - Spam Probability: [0.9998997]\n",
      "Spam\n",
      "Text: (Bank of Granite issues Strong-Buy) EXPLOSIVE PICK FOR OUR MEMBERS *****UP OVER 300% *********** Nasdaq Symbol CDGT That is a $5.00 per.. - Spam Probability: [0.7424263]\n",
      "Spam\n",
      "Text: &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF. - Spam Probability: [0.00232021]\n",
      "Non-Spam\n",
      "Text: pay 100 dolars - Spam Probability: [0.655978]\n",
      "Spam\n"
     ]
    }
   ],
   "source": [
    "# Assuming x_train is your training data\n",
    "text_data = [\"Congratulations! You've won a free cruise. Click the link to claim your prize.\",\n",
    "             \"Hi there, just checking in. How are you doing?\",\"You are a great role model\",\n",
    "             \"100 dating service cal;l 09064012103 box334sk38ch\",\n",
    "            \"(Bank of Granite issues Strong-Buy) EXPLOSIVE PICK FOR OUR MEMBERS *****UP OVER 300% *********** Nasdaq Symbol CDGT That is a $5.00 per..\",\n",
    "            \"&lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\",\"pay 100 dolars\"]\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "max_len = 200  # Assuming a max sequence length of 200\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Convert characters in text to ordinals\n",
    "max_chars = 200  # Assuming a max character length of 200\n",
    "char_tokenizer_wrapper = lambda x: [ord(char) for char in x]\n",
    "text_char_seq = list(map(char_tokenizer_wrapper, text_data))\n",
    "text_char_padded = pad_sequences(text_char_seq, maxlen=max_chars)\n",
    "\n",
    "# Repeat the samples in text_char_padded to match the number of samples in padded_sequences\n",
    "num_repeats = len(padded_sequences) // len(text_char_padded)\n",
    "text_char_padded_repeated = np.repeat(text_char_padded, num_repeats, axis=0)\n",
    "\n",
    "# Adjust the number of samples in text_char_padded to match the number of samples in padded_sequences\n",
    "num_additional_samples = len(padded_sequences) - len(text_char_padded)\n",
    "additional_samples_indices = np.random.choice(len(text_char_padded), num_additional_samples)\n",
    "text_char_padded_adj = np.concatenate((text_char_padded, text_char_padded[additional_samples_indices]), axis=0)\n",
    "\n",
    "# Assuming you have the model loaded here\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = model.predict([padded_sequences, text_char_padded_adj])\n",
    "\n",
    "# Print the predictions for each input text\n",
    "for i, text in enumerate(text_data):\n",
    "    print(f'Text: {text} - Spam Probability: {predictions[i]}')\n",
    "    \n",
    "    if predictions[i] <= 0.2:\n",
    "        print('Non-Spam')\n",
    "    else:\n",
    "        print('Spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fdb66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gerty",
   "language": "python",
   "name": "gerty"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
